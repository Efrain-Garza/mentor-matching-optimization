{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "865f19d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PyTorch Benchmark (Matrix Multiply) ===\n",
      "CPU time: 0.0755 s\n",
      "GPU time: 0.0041 s\n",
      "Speed-up: 18.36×\n",
      "\n",
      "=== CuPy Benchmark (Monte Carlo Simulation) ===\n",
      "CPU time: 0.0922 s\n",
      "GPU time: 0.0037 s\n",
      "Speed-up: 25.20×\n",
      "\n",
      "Benchmark Summary:\n",
      "                  Benchmark  CPU Time (s)  GPU Time (s)  Speed-up (×)\n",
      "    PyTorch Matrix Multiply      0.075536      0.004114     18.362177\n",
      "CuPy Monte Carlo Simulation      0.092209      0.003660     25.195570\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Appendix A.0.1 — GPU Validation and Performance Benchmark\n",
    "# ------------------------------------------------------------\n",
    "# Author: Efrain G. Garza\n",
    "# Research Context:\n",
    "#   Mentor–Mentee Optimization (Euclidean Distance Modeling,\n",
    "#   Vectorized Degree Representations, and Linear Algebra Acceleration)\n",
    "# Institution: University of Texas at San Antonio\n",
    "#\n",
    "# Purpose:\n",
    "#   This appendix provides an empirical validation of GPU acceleration\n",
    "#   for the computational components used in this research. Two core\n",
    "#   workloads are benchmarked on both CPU and GPU:\n",
    "#\n",
    "#     (1) Large-scale matrix multiplication (PyTorch), representative of\n",
    "#         the vector–matrix operations underlying the Euclidean distance\n",
    "#         matrix in the mentor–mentee optimization model.\n",
    "#\n",
    "#     (2) High-dimensional simulation (CuPy), reflecting the iterative\n",
    "#         GPU-based procedures used during model diagnostics,\n",
    "#         sensitivity checks, and distance-matrix verification.\n",
    "#\n",
    "#   The results confirm that the NVIDIA RTX 3070 Ti Laptop GPU provides\n",
    "#   an ~18× acceleration relative to CPU execution, ensuring that all\n",
    "#   computational experiments in this study were conducted under a\n",
    "#   validated and reproducible hardware–software environment.\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PyTorch GPU vs CPU — Matrix Multiplication Benchmark\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n=== PyTorch Benchmark (Matrix Multiply) ===\")\n",
    "\n",
    "N = 4000  # 4000×4000 matrices\n",
    "\n",
    "# CPU computation\n",
    "a_cpu = torch.rand(N, N)\n",
    "b_cpu = torch.rand(N, N)\n",
    "\n",
    "start = time.time()\n",
    "_ = torch.mm(a_cpu, b_cpu)\n",
    "cpu_time_torch = time.time() - start\n",
    "\n",
    "# GPU computation\n",
    "device_gpu = torch.device(\"cuda\")\n",
    "a_gpu = a_cpu.to(device_gpu)\n",
    "b_gpu = b_cpu.to(device_gpu)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "start = time.time()\n",
    "_ = torch.mm(a_gpu, b_gpu)\n",
    "torch.cuda.synchronize()\n",
    "gpu_time_torch = time.time() - start\n",
    "\n",
    "print(f\"CPU time: {cpu_time_torch:.4f} s\")\n",
    "print(f\"GPU time: {gpu_time_torch:.4f} s\")\n",
    "print(f\"Speed-up: {cpu_time_torch / gpu_time_torch:.2f}×\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CuPy GPU vs CPU — Monte Carlo Simulation Benchmark\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n=== CuPy Benchmark (Monte Carlo Simulation) ===\")\n",
    "\n",
    "N = 8_000_000  # 8 million draws\n",
    "\n",
    "# ---- CPU version (NumPy) ----\n",
    "start = time.time()\n",
    "x_cpu = np.random.randn(N)\n",
    "sum_cpu = x_cpu.sum()\n",
    "cpu_time_sim = time.time() - start\n",
    "\n",
    "# ---- GPU version (CuPy) ----\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "start = time.time()\n",
    "x_gpu = cp.random.randn(N)\n",
    "sum_gpu = cp.sum(x_gpu)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "gpu_time_sim = time.time() - start\n",
    "\n",
    "print(f\"CPU time: {cpu_time_sim:.4f} s\")\n",
    "print(f\"GPU time: {gpu_time_sim:.4f} s\")\n",
    "print(f\"Speed-up: {cpu_time_sim / gpu_time_sim:.2f}×\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Benchmark Summary Table (for the Appendix)\n",
    "# ------------------------------------------------------------\n",
    "results = pd.DataFrame({\n",
    "    \"Benchmark\": [\n",
    "        \"PyTorch Matrix Multiply\",\n",
    "        \"CuPy Monte Carlo Simulation\"\n",
    "    ],\n",
    "    \"CPU Time (s)\": [\n",
    "        cpu_time_torch,\n",
    "        cpu_time_sim\n",
    "    ],\n",
    "    \"GPU Time (s)\": [\n",
    "        gpu_time_torch,\n",
    "        gpu_time_sim\n",
    "    ],\n",
    "    \"Speed-up (×)\": [\n",
    "        cpu_time_torch / gpu_time_torch,\n",
    "        cpu_time_sim / gpu_time_sim\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nBenchmark Summary:\")\n",
    "print(results.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
